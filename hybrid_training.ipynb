{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:83: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:86: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as tfunc\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn.functional as func\n",
    "\n",
    "from sklearn.metrics.ranking import roc_auc_score\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from models.chexnet.DensenetModels import DenseNet121\n",
    "from models.models import ResNet18\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "writer = SummaryWriter('./logs')\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Read images and corresponding labels.\n",
    "\"\"\"\n",
    "class ChestXrayDataSet(Dataset):\n",
    "    \n",
    "    def convert_to_ones(self, df, disease):\n",
    "        df[disease] = df[disease].replace([-1.0], 1.0)\n",
    "    \n",
    "    def convert_to_zeros(self, df, disease):\n",
    "        df[disease] = df[disease].replace([-1.0], 0.0)\n",
    "        \n",
    "    def convert_to_multi(self, df, disease):\n",
    "        df[disease] = df[disease].replace([-1.0], 2.0)\n",
    "\n",
    "    def __init__(self, data_dir, image_list_file, diseases=['Atelectasis', 'Consolidation', 'Edema','Cardiomegaly', 'Pleural Effusion'], side='Frontal', transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_dir: path to image directory.\n",
    "            image_list_file: path to the file containing images\n",
    "                with corresponding labels.\n",
    "            transform: optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        image_names = []\n",
    "        labels = []\n",
    "        chex_df = pd.read_csv(image_list_file)\n",
    "        chex_df = chex_df.fillna(0.0)\n",
    "        chex_df = chex_df.loc[chex_df['Frontal/Lateral'] == side]\n",
    "        self.convert_to_ones(chex_df, 'Atelectasis')\n",
    "        self.convert_to_ones(chex_df, 'Consolidation')\n",
    "        self.convert_to_ones(chex_df, 'Edema')\n",
    "        self.convert_to_multi(chex_df, 'Cardiomegaly')\n",
    "        self.convert_to_multi(chex_df, 'Pleural Effusion')\n",
    "\n",
    "#         chex_df_diseases = chex_df[diseases]\n",
    "                         \n",
    "#         if 'train' in image_list_file:\n",
    "#             chex_df = chex_df\n",
    "#         if len(diseases) == 1:\n",
    "#             chex_df = chex_df.loc[chex_df['Pleural Effusion'] != -1] #U-Ignore\n",
    "#         print(chex_df)\n",
    "        labels = chex_df.as_matrix(columns=diseases)\n",
    "        labels = list(labels)\n",
    "\n",
    "        image_names = chex_df.as_matrix(columns=['Path']).flatten()\n",
    "        image_names = [os.path.join(data_dir, im_name) for im_name in image_names]\n",
    "\n",
    "        self.image_names = image_names\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index: the index of item\n",
    "        Returns:\n",
    "            image and its labels\n",
    "        \"\"\"\n",
    "        side_image_name = self.image_names[index]\n",
    "        front_image_name = side_image_name.replace('2_lateral', '1_frontal')\n",
    "        side_image = Image.open(side_image_name).convert('RGB')\n",
    "        front_image = Image.open(front_image_name).convert('RGB')\n",
    "        label = torch.FloatTensor(self.labels[index])\n",
    "        if self.transform is not None:\n",
    "            side_image = self.transform(side_image)\n",
    "            front_image = self.transform(front_image)\n",
    "        return front_image, side_image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "class ChexnetTrainer():\n",
    "\n",
    "    #---- Train the densenet network \n",
    "    #---- pathDirData - path to the directory that contains images\n",
    "    #---- pathFileTrain - path to the file that contains image paths and label pairs (training set)\n",
    "    #---- pathFileVal - path to the file that contains image path and label pairs (validation set)\n",
    "    #---- nnArchitecture - model architecture 'DENSE-NET-121', 'DENSE-NET-169' or 'DENSE-NET-201'\n",
    "    #---- nnIsTrained - if True, uses pre-trained version of the network (pre-trained on imagenet)\n",
    "    #---- nnClassCount - number of output classes \n",
    "    #---- trBatchSize - batch size\n",
    "    #---- trMaxEpoch - number of epochs\n",
    "    #---- transResize - size of the image to scale down to (not used in current implementation)\n",
    "    #---- transCrop - size of the cropped image \n",
    "    #---- launchTimestamp - date/time, used to assign unique name for the checkpoint file\n",
    "    #---- checkpoint - if not None loads the model and continues training\n",
    "    #--- classes - is the number of classes to predict (Note =/= final layer of Dense Net) -- Saj\n",
    "    \n",
    "    \n",
    "    def train (pathDirData, pathFileTrain, pathFileVal, nnArchitecture, nnIsTrained, nnClassCount, trBatchSize, trMaxEpoch, transResize, transCrop, launchTimestamp, checkpoint,classes):\n",
    "        #------------------  Special Loss \n",
    "        # Takes in Logits, except 0,1,2 --> logits => sigmoid\n",
    "        # returns multi label loss\n",
    "        def lossCriterion(varOutput,varTarget):\n",
    "            CEloss =  torch.nn.CrossEntropyLoss()\n",
    "            BCEloss = torch.nn.BCELoss()\n",
    "\n",
    "            L1 = BCEloss(varOutput[:,0],varTarget[:,0]) \n",
    "            L2 = BCEloss(varOutput[:,1],varTarget[:,1])\n",
    "            L3 = BCEloss(varOutput[:,2],varTarget[:,2])\n",
    "            varTarget = varTarget.long()\n",
    "            L4 = CEloss(varOutput[:,3:6],varTarget[:,3])\n",
    "            L5 = CEloss(varOutput[:,6:9],varTarget[:,4])\n",
    "\n",
    "            \n",
    "            lossvalue = (L1 + L2 + L3 + L4 + L5)/5\n",
    "            \n",
    "            return lossvalue\n",
    "        \n",
    "        \n",
    "        #-------------------- SETTINGS: NETWORK ARCHITECTURE\n",
    "        if nnArchitecture == 'DENSE-NET-121': model = DenseNet121(nnClassCount, nnIsTrained).cuda()\n",
    "        elif nnArchitecture == 'DENSE-NET-169': model = DenseNet169(nnClassCount, nnIsTrained).cuda()\n",
    "        elif nnArchitecture == 'DENSE-NET-201': model = DenseNet201(nnClassCount, nnIsTrained).cuda()\n",
    "        elif nnArchitecture == 'RES-NET-18': model = ResNet18(nnClassCount, nnIsTrained).cuda()\n",
    "        \n",
    "        model = torch.nn.DataParallel(model).cuda()\n",
    "       \n",
    "        #-------------------- SETTINGS: DATA TRANSFORMS |TRAIN|\n",
    "        normalize = transforms.Normalize([0.50616586, 0.50616586, 0.50616586], [0.2879059, 0.2879059, 0.2879059]) #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        \n",
    "        transformList = []\n",
    "        transformList.append(transforms.Resize(transResize))\n",
    "        transformList.append(transforms.ToTensor())\n",
    "        transformList.append(normalize)    \n",
    "        transformSequence=transforms.Compose(transformList)\n",
    "\n",
    "        #-------------------- SETTINGS: DATASET BUILDER |TRAIN|\n",
    "                    \n",
    "        datasetTrain = ChestXrayDataSet(data_dir=pathDirData,image_list_file=pathFileTrain, side='Lateral' transform=transformSequence)              \n",
    "        dataLoaderTrain = DataLoader(dataset=datasetTrain, batch_size=trBatchSize, shuffle=True,  num_workers=0, pin_memory=False)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #-------------------- SETTINGS: DATA TRANSFORMS, TEN CROPS |VAL|\n",
    "\n",
    "        \n",
    "        #-------------------- SETTINGS: DATASET BUILDERS |VAL|\n",
    "        datasetVal =   ChestXrayDataSet(data_dir=pathDirData, image_list_file=pathFileVal, side='Lateral', transform=transformSequence)\n",
    "        dataLoaderVal = DataLoader(dataset=datasetVal, batch_size=trBatchSize, shuffle=False, num_workers=0, pin_memory=False)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #-------------------- SETTINGS: OPTIMIZER & SCHEDULER\n",
    "        optimizer = optim.Adam (model.parameters(), lr=0.0001, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-5)\n",
    "        scheduler = ReduceLROnPlateau(optimizer, factor = 0.1, patience = 5, mode = 'min')\n",
    "\n",
    "        #-------------------- SETTINGS: LOSS\n",
    "        loss = lossCriterion\n",
    "       \n",
    "        counter = 0\n",
    "        \n",
    "\t#---- Load checkpoint \n",
    "        if checkpoint != None:\n",
    "            modelCheckpoint = torch.load(checkpoint)\n",
    "            model.load_state_dict(modelCheckpoint['state_dict'])\n",
    "            optimizer.load_state_dict(modelCheckpoint['optimizer'])\n",
    "            counter = modelCheckpoint['counter']\n",
    "        \n",
    "        #---- TRAIN THE NETWORK\n",
    "        lossMIN = 100000\n",
    "        \n",
    "        for epochID in range (0, trMaxEpoch):\n",
    "            \n",
    "            timestampTime = time.strftime(\"%H%M%S\")\n",
    "            timestampDate = time.strftime(\"%d%m%Y\")\n",
    "            timestampSTART = timestampDate + '-' + timestampTime\n",
    "                         \n",
    "            lossTrain, counter = ChexnetTrainer.epochTrain (model, dataLoaderTrain, dataLoaderVal, optimizer, scheduler, trMaxEpoch, nnClassCount, loss, counter,classes)\n",
    "            lossVal, losstensor, __ = ChexnetTrainer.epochVal (model, dataLoaderVal, optimizer, scheduler, trMaxEpoch, nnClassCount, loss, counter,classes)\n",
    "            \n",
    "            timestampTime = time.strftime(\"%H%M%S\")\n",
    "            timestampDate = time.strftime(\"%d%m%Y\")\n",
    "            timestampEND = timestampDate + '-' + timestampTime\n",
    "\n",
    "            scheduler.step(losstensor.item())\n",
    "            writer.add_scalar('logs/train_loss_epoch', lossTrain, epochID)\n",
    "            writer.add_scalar('logs/val_loss_epoch', lossVal, epochID)\n",
    "            if lossVal < lossMIN:\n",
    "\n",
    "                lossMIN = lossVal    \n",
    "                torch.save({'epoch': epochID + 1, 'state_dict': model.state_dict(), 'best_loss': lossMIN, 'optimizer' : optimizer.state_dict()}, './forward/m-' + launchTimestamp + '.pth.tar')\n",
    "                print ('Epoch [' + str(epochID + 1) + '] [save] [' + timestampEND + '] loss= ' + str(lossVal))\n",
    "            else:\n",
    "                print ('Epoch [' + str(epochID + 1) + '] [----] [' + timestampEND + '] loss= ' + str(lossVal))\n",
    "                     \n",
    "    #-------------------------------------------------------------------------------- \n",
    "       \n",
    "    def epochTrain (model, dataLoader, dataLoaderVal, optimizer, scheduler, epochMax, classCount, loss, counter,classes):\n",
    "        \n",
    "        model.train()\n",
    "        lossTrain = 0\n",
    "        lossTrainNorm = 0\n",
    "        \n",
    "        avg_loss = 0.0\n",
    "\n",
    "        for batchID, (input, target) in enumerate (dataLoader):\n",
    "\n",
    "            target = target.cuda()\n",
    "            varInput = torch.autograd.Variable(input)\n",
    "            varTarget = torch.autograd.Variable(target)         \n",
    "            varOutput = model(varInput)\n",
    "\n",
    "\n",
    "            varOutput[:,0] = torch.sigmoid(varOutput[:,0])\n",
    "            varOutput[:,1] = torch.sigmoid(varOutput[:,1])\n",
    "            varOutput[:,2] = torch.sigmoid(varOutput[:,2])\n",
    "\n",
    "            lossvalue = loss(varOutput,varTarget)\n",
    "\n",
    "            avg_loss = avg_loss * (batchID)/(batchID+1) + lossvalue * 1.0/(batchID+ 1)\n",
    "            lossTrain += lossvalue\n",
    "            lossTrainNorm += 1\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            lossvalue.backward()\n",
    "            optimizer.step()\n",
    "            writer.add_scalar('logs/train_loss', avg_loss, counter)\n",
    "            if batchID % 200 == 0:\n",
    "                ChexnetTrainer.epochVal(model, dataLoaderVal, optimizer, scheduler, epochMax, classCount, loss, counter,classes)\n",
    "                print('Loss:' + str(avg_loss.item()))\n",
    "            if batchID % 2400 == 0:\n",
    "                __, __, aurocMean = ChexnetTrainer.epochVal(model, dataLoaderVal, optimizer, scheduler, epochMax, classCount, loss, counter,classes)\n",
    "                torch.save({'counter' : counter, 'state_dict': model.state_dict(), 'valAUROC' : aurocMean , 'optimizer' : optimizer.state_dict()}, './forward/m-' + str(counter) + '_' + str(round(aurocMean, 3)) + '.pth.tar')\n",
    "\n",
    "                \n",
    "#             print(counter)\n",
    "            counter += 1\n",
    "\n",
    "        outLoss = lossTrain/lossTrainNorm\n",
    "        return outLoss, counter\n",
    "\n",
    "                        \n",
    "    #-------------------------------------------------------------------------------- \n",
    "        \n",
    "    def epochVal (model, dataLoader, optimizer, scheduler, epochMax, classCount, loss, counter,classes):\n",
    "        \n",
    "        print('epoc val')\n",
    "        model.eval()\n",
    "        \n",
    "        lossVal = 0\n",
    "        lossValNorm = 0\n",
    "        losstensorMean = 0\n",
    "\n",
    "        outGT = torch.FloatTensor().cuda()\n",
    "        outPRED = torch.FloatTensor().cuda()\n",
    "        with torch.no_grad():\n",
    "            for i, (input, target) in enumerate(dataLoader):\n",
    "                #Val code\n",
    "                target = target.cuda()\n",
    "                varInput = torch.autograd.Variable(input.cuda())\n",
    "                varTarget = torch.autograd.Variable(target)\n",
    "                varOutput = model(varInput)\n",
    "\n",
    "                varOutput[:,0] = torch.sigmoid(varOutput[:,0])\n",
    "                varOutput[:,1] = torch.sigmoid(varOutput[:,1])\n",
    "                varOutput[:,2] = torch.sigmoid(varOutput[:,2])            \n",
    "\n",
    "\n",
    "                ### VAL Preds for AUROC\n",
    "                bPRED = torch.zeros(varOutput.shape[0], 5).cuda()\n",
    "                bPRED[:,0] = varOutput[:,0]\n",
    "                bPRED[:,1] = varOutput[:,1]\n",
    "                bPRED[:,2] = varOutput[:,2]\n",
    "                \n",
    "                soft_a = torch.nn.functional.softmax(varOutput[:,3:6], dim=-1).data\n",
    "\n",
    "                a0, a1, a2 = soft_a[:, 0], soft_a[:, 1], soft_a[:, 2]\n",
    "                bPRED[:, 3] = a1/(a0+a1)\n",
    "                soft_b = torch.nn.functional.softmax(varOutput[:,6:9], dim=-1).data\n",
    "                b0, b1, b2 = soft_b[:, 0], soft_b[:, 1], soft_b[:, 2]\n",
    "                bPRED[:, 4] = b1/(b0+b1)\n",
    "\n",
    "                outPRED = torch.cat((outPRED, bPRED.data), 0)            \n",
    "                outGT = torch.cat((outGT, target), 0)\n",
    "\n",
    "\n",
    "                losstensor = loss(varOutput,varTarget)\n",
    "\n",
    "                losstensorMean += losstensor\n",
    "                lossVal += losstensor.item()\n",
    "                lossValNorm += 1\n",
    "                ##block comment was here\n",
    "\n",
    "            outLoss = lossVal / lossValNorm\n",
    "            losstensorMean = losstensorMean / lossValNorm\n",
    "\n",
    "            aurocIndividual = ChexnetTrainer.computeAUROC(outGT, outPRED, classes)\n",
    "            aurocMean = np.array(aurocIndividual).mean()\n",
    "\n",
    "            print(\"AUROC val\", aurocMean)\n",
    "            print(\"AUROC all\", aurocIndividual)\n",
    "            writer.add_scalar('logs/val_auroc', aurocMean, counter)\n",
    "\n",
    "        return outLoss, losstensorMean, aurocMean            \n",
    "#             outGT = torch.cat((outGT, target), 0)\n",
    "# outPred = empty variable\n",
    "\n",
    "\n",
    "##varMean to varOutput\n",
    "#             outPRED = torch.zeros(out.shape[0], 5).cuda()\n",
    "#             outPRED[:,0] = outMean[:,0]\n",
    "#             outPRED[:,1] = outMean[:,1]\n",
    "#             outPRED[:,2] = outMean[:,2]\n",
    "#             outPRED[:,3] = torch.max(outMean[:,3:6],1)[0]\n",
    "#             outPRED[:,4] = torch.max(outMean[:,6:9],1)[0]\n",
    "\n",
    "# #             outPRED = torch.cat((outPRED, outMean.data), 0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "############            \n",
    "#             outGT = torch.cat((outGT, target), 0)\n",
    "            \n",
    "#             bs, c, h, w = input.size()\n",
    "\n",
    "#             varInput = torch.autograd.Variable(input.view(-1, c, h, w).cuda(), volatile=True)\n",
    "            \n",
    "#             out = model(varInput)\n",
    "#             outMean = out.view(bs, -1)\n",
    "    \n",
    "#             outPRED = torch.zeros(out.shape[0], 5).cuda()\n",
    "#             outPRED[:,0] = outMean[:,0]\n",
    "#             outPRED[:,1] = outMean[:,1]\n",
    "#             outPRED[:,2] = outMean[:,2]\n",
    "#             outPRED[:,3] = torch.max(outMean[:,3:6],1)[0]\n",
    "#             outPRED[:,4] = torch.max(outMean[:,6:9],1)[0]\n",
    "            \n",
    "            \n",
    "# #             outPRED = torch.cat((outPRED, outMean.data), 0)\n",
    "            \n",
    "#             varOutput = outPRED\n",
    "#             varTarget = outGT\n",
    "            \n",
    "# #             losstensor = loss(varOutput, varTarget)\n",
    "\n",
    "#             CEloss =  torch.nn.CrossEntropyLoss()\n",
    "#             BCEloss = torch.nn.BCELoss()\n",
    "\n",
    "# #             varTarget = varTarget.type(torch.long)\n",
    "#             L1 = BCEloss(varOutput[:,:1],varTarget[:,0]) \n",
    "#             L2 = BCEloss(varOutput[:,1:2],varTarget[:,1])\n",
    "#             L3 = BCEloss(varOutput[:,2:3],varTarget[:,2])\n",
    "#             varTarget = varTarget.long()\n",
    "#             L4 = CEloss(varOutput[:,3:6],varTarget[:,3])\n",
    "#             L5 = CEloss(varOutput[:,6:9],varTarget[:,4])\n",
    "\n",
    "            \n",
    "#             losstensor = L1 + L2 + L3 + L4 + L5\n",
    "#             losstensor /= 5\n",
    "\n",
    "\n",
    "#             losstensorMean += losstensor\n",
    "#             lossVal += losstensor.item()\n",
    "#             lossValNorm += 1\n",
    "            \n",
    "\n",
    "               \n",
    "    #--------------------------------------------------------------------------------     \n",
    "     \n",
    "    #---- Computes area under ROC curve \n",
    "    #---- dataGT - ground truth data\n",
    "    #---- dataPRED - predicted data\n",
    "    #---- classCount - number of classes\n",
    "    \n",
    "    def computeAUROC (dataGT, dataPRED, classCount):\n",
    "        \n",
    "        outAUROC = []\n",
    "        \n",
    "        datanpGT = dataGT.cpu().numpy()\n",
    "        datanpPRED = dataPRED.cpu().numpy()\n",
    "        \n",
    "        for i in range(classCount):\n",
    "            outAUROC.append(roc_auc_score(datanpGT[:, i], datanpPRED[:, i]))\n",
    "            \n",
    "        return outAUROC\n",
    "        \n",
    "        \n",
    "    #--------------------------------------------------------------------------------  \n",
    "    \n",
    "    #---- Test the trained network \n",
    "    #---- pathDirData - path to the directory that contains images\n",
    "    #---- pathFileTrain - path to the file that contains image paths and label pairs (training set)\n",
    "    #---- pathFileVal - path to the file that contains image path and label pairs (validation set)\n",
    "    #---- nnArchitecture - model architecture 'DENSE-NET-121', 'DENSE-NET-169' or 'DENSE-NET-201'\n",
    "    #---- nnIsTrained - if True, uses pre-trained version of the network (pre-trained on imagenet)\n",
    "    #---- nnClassCount - number of output classes \n",
    "    #---- trBatchSize - batch size\n",
    "    #---- trMaxEpoch - number of epochs\n",
    "    #---- transResize - size of the image to scale down to (not used in current implementation)\n",
    "    #---- transCrop - size of the cropped image \n",
    "    #---- launchTimestamp - date/time, used to assign unique name for the checkpoint file\n",
    "    #---- checkpoint - if not None loads the model and continues training\n",
    "    \n",
    "    def test (pathDirData, pathFileTest, pathModel, nnArchitecture, nnClassCount, nnIsTrained, trBatchSize, transResize, transCrop, launchTimeStamp):   \n",
    "        \n",
    "        \n",
    "        CLASS_NAMES = [ 'Atelectasis', 'Cardiomegaly', 'Effusion', 'Infiltration', 'Mass', 'Nodule', 'Pneumonia',\n",
    "                'Pneumothorax', 'Consolidation', 'Edema', 'Emphysema', 'Fibrosis', 'Pleural_Thickening', 'Hernia']\n",
    "        \n",
    "        cudnn.benchmark = True\n",
    "        \n",
    "        #-------------------- SETTINGS: NETWORK ARCHITECTURE, MODEL LOAD\n",
    "        if nnArchitecture == 'DENSE-NET-121': model = DenseNet121(nnClassCount, nnIsTrained).cuda()\n",
    "        elif nnArchitecture == 'DENSE-NET-169': model = DenseNet169(nnClassCount, nnIsTrained).cuda()\n",
    "        elif nnArchitecture == 'DENSE-NET-201': model = DenseNet201(nnClassCount, nnIsTrained).cuda()\n",
    "        \n",
    "        model = torch.nn.DataParallel(model).cuda() \n",
    "        \n",
    "        modelCheckpoint = torch.load(pathModel)\n",
    "        model.load_state_dict(modelCheckpoint['state_dict'])\n",
    "\n",
    "        #-------------------- SETTINGS: DATA TRANSFORMS, TEN CROPS\n",
    "        normalize = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        \n",
    "        #-------------------- SETTINGS: DATASET BUILDERS\n",
    "        transformList = []\n",
    "        transformList.append(transforms.Resize(transResize))\n",
    "        transformList.append(transforms.TenCrop(transCrop))\n",
    "        transformList.append(transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops])))\n",
    "        transformList.append(transforms.Lambda(lambda crops: torch.stack([normalize(crop) for crop in crops])))\n",
    "        transformSequence=transforms.Compose(transformList)\n",
    "        \n",
    "        datasetTest = DatasetGenerator(pathImageDirectory=pathDirData, pathDatasetFile=pathFileTest, transform=transformSequence)\n",
    "        dataLoaderTest = DataLoader(dataset=datasetTest, batch_size=trBatchSize, num_workers=0, shuffle=False, pin_memory=False)\n",
    "        \n",
    "        outGT = torch.FloatTensor().cuda()\n",
    "        outPRED = torch.FloatTensor().cuda()\n",
    "       \n",
    "        model.eval()\n",
    "        \n",
    "        for i, (input, target) in enumerate(dataLoaderTest):\n",
    "            \n",
    "            target = target.cuda()\n",
    "            outGT = torch.cat((outGT, target), 0)\n",
    "            \n",
    "            bs, n_crops, c, h, w = input.size()\n",
    "            \n",
    "            varInput = torch.autograd.Variable(input.view(-1, c, h, w).cuda())\n",
    "            \n",
    "            out = model(varInput)\n",
    "            outMean = out.view(bs, n_crops, -1).mean(1)\n",
    "            \n",
    "            outPRED = torch.cat((outPRED, outMean.data), 0)\n",
    "\n",
    "        aurocIndividual = ChexnetTrainer.computeAUROC(outGT, outPRED, nnClassCount)\n",
    "        aurocMean = np.array(aurocIndividual).mean()\n",
    "        \n",
    "        print ('AUROC mean ', aurocMean)\n",
    "        \n",
    "        for i in range (0, len(aurocIndividual)):\n",
    "            print (CLASS_NAMES[i], ' ', aurocIndividual[i])\n",
    "        \n",
    "     \n",
    "        return\n",
    "#-------------------------------------------------------------------------------- \n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "DATA_DIR = './data'\n",
    "TRAIN_IMAGE_LIST = './data/CheXpert-v1.0-small/train.csv'\n",
    "VAL_IMAGE_LIST = './data/CheXpert-v1.0-small/valid.csv'\n",
    "valid_dataset = ChestXrayDataSet(data_dir=DATA_DIR,\n",
    "                                image_list_file=VAL_IMAGE_LIST)\n",
    "\n",
    "nnIsTrained = True\n",
    "nnArchitecture = 'DENSE-NET-121'\n",
    "\n",
    "nnClassCount = 9\n",
    "classes = 5\n",
    "\n",
    "trBatchSize = 32\n",
    "trMaxEpoch = 50\n",
    "transResize = (300, 300)\n",
    "transCrop = 224\n",
    "launchTimestamp = ''\n",
    "checkpoint = None\n",
    "#ChexnetTrainer.train(DATA_DIR,TRAIN_IMAGE_LIST,VAL_IMAGE_LIST,nnArchitecture, nnIsTrained, nnClassCount, trBatchSize, trMaxEpoch, transResize, transCrop, launchTimestamp, checkpoint,classes)\n",
    "\n",
    "\n",
    "# In[ ]:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = 'forward'\n",
    "model1 = DenseNet121(9, False).cuda()\n",
    "model1 = torch.nn.DataParallel(model1).cuda()\n",
    "checkpoint = torch.load(os.path.join(f, 'm-19140_0.889.pth.tar'))\n",
    "model1.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "model2 = DenseNet121(9, False).cuda()\n",
    "model2 = torch.nn.DataParallel(model2).cuda()\n",
    "checkpoint = torch.load(os.path.join(f, 'm-37050_0.897.pth.tar'))\n",
    "model2.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 1024])\n"
     ]
    }
   ],
   "source": [
    "print(list(model1.parameters())[-2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
