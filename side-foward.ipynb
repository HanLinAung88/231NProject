{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "import copy\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as tfunc\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn.functional as func\n",
    "\n",
    "from sklearn.metrics.ranking import roc_auc_score\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from models.chexnet.DensenetModels import DenseNet121\n",
    "from models.models import ResNet18\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'checkpoint' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4e7998386cda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDenseNet121\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnnClassCount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodelCheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelCheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbetas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.999\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-08\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'checkpoint' is not defined"
     ]
    }
   ],
   "source": [
    "nnClassCount = 9\n",
    "model = DenseNet121(nnClassCount, False).cuda()\n",
    "model = torch.nn.DataParallel(model).cuda()\n",
    "modelCheckpoint = torch.load(checkpoint)\n",
    "model.load_state_dict(modelCheckpoint['state_dict'])\n",
    "optimizer = optim.Adam (model.parameters(), lr=0.0001, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-5)\n",
    "optimizer.load_state_dict(modelCheckpoint['optimizer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class forward_side(nn.Module):\n",
    "    def __init__(self, nnClassCount, forward_cp, side_cp):\n",
    "        super(forward_side, self).__init__()\n",
    "        self.forwardModel = DenseNet121(nnClassCount, False)\n",
    "        self.sideModel = DenseNet121(nnClassCount, False)\n",
    "        \n",
    "        Fstate_dict = self.change_state_dict_keys(torch.load(forward_cp)['state_dict'])\n",
    "        Cstate_dict = self.change_state_dict_keys(torch.load(side_cp)['state_dict'])\n",
    "        self.forwardModel.load_state_dict(Fstate_dict)\n",
    "        self.sideModel.load_state_dict(Cstate_dict)\n",
    "        \n",
    "#         for i, param in enumerate(self.forwardModel.parameters()):\n",
    "#             if i < len(list(self.forwardModel.parameters())) - 5:\n",
    "#                 param.requires_grad = False\n",
    "#         for i, param in enumerate(self.sideModel.parameters()):\n",
    "#             if i < len(list(self.sideModel.parameters())) - 5:\n",
    "#                 param.requires_grad = False\n",
    "        \n",
    "        for param in self.forwardModel.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.sideModel.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.FkernelCount = self.forwardModel.densenet121.classifier.in_features\n",
    "        self.SkernelCount = self.sideModel.densenet121.classifier.in_features\n",
    "        self.forwardModel.densenet121.classifier = nn.Identity()\n",
    "        self.sideModel.densenet121.classifier = nn.Identity()\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.FkernelCount+self.SkernelCount,500)\n",
    "        self.fc2 = nn.Linear(500,100)\n",
    "        self.fc3 = nn.Linear(100,nnClassCount)\n",
    "        \n",
    "    def change_state_dict_keys(self,state_dict):\n",
    "        keys = state_dict.keys()\n",
    "        new_state_dict = {}\n",
    "        for key in keys:\n",
    "            clean_k = '.'.join(key.split('.')[1:]) # module.dense121.conv0.weight -> dense121.conv0.weight\n",
    "            new_state_dict[clean_k] = state_dict[key]\n",
    "        return new_state_dict\n",
    "        \n",
    "    def forward(self, xF,xS):\n",
    "        xF = self.forwardModel(xF)\n",
    "        xS = self.sideModel(xS)\n",
    "        x = torch.cat((xF,xS),-1)\n",
    "        x = func.relu(self.fc1(x))\n",
    "        x = func.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fcheckpoint = './best_models/forward121/m-37050_0-Copy1.897.pth.tar'\n",
    "Scheckpoint = './best_models/lateral/m-9117_0.882.pth.tar'\n",
    "nnClassCount = 9\n",
    "model = forward_side(nnClassCount,Fcheckpoint,Scheckpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:99: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:102: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoc val\n",
      "AUROC val 0.7363861080212581\n",
      "AUROC all [0.8444444444444444, 0.8064516129032258, 0.8390804597701149, 0.43333333333333335, 0.7586206896551724]\n",
      "Loss:0.8586355447769165\n",
      "epoc val\n",
      "AUROC val 0.7363861080212581\n",
      "AUROC all [0.8444444444444444, 0.8064516129032258, 0.8390804597701149, 0.43333333333333335, 0.7586206896551724]\n",
      "epoc val\n",
      "AUROC val 0.8757982120051085\n",
      "AUROC all [0.837037037037037, 1.0, 0.896551724137931, 0.7833333333333333, 0.8620689655172413]\n",
      "Loss:0.6129118800163269\n",
      "epoc val\n",
      "AUROC val 0.8757982120051085\n",
      "AUROC all [0.837037037037037, 1.0, 0.896551724137931, 0.7833333333333333, 0.8620689655172413]\n",
      "epoc val\n",
      "AUROC val 0.8994380587484037\n",
      "AUROC all [0.8592592592592593, 1.0, 0.9310344827586207, 0.8333333333333333, 0.8735632183908046]\n",
      "Loss:0.5259038209915161\n",
      "epoc val\n",
      "AUROC val 0.8994380587484037\n",
      "AUROC all [0.8592592592592593, 1.0, 0.9310344827586207, 0.8333333333333333, 0.8735632183908046]\n",
      "epoc val\n",
      "AUROC val 0.9200383141762453\n",
      "AUROC all [0.8444444444444446, 1.0, 0.9310344827586207, 0.9166666666666667, 0.9080459770114943]\n",
      "Loss:0.48886293172836304\n",
      "epoc val\n",
      "AUROC val 0.9200383141762453\n",
      "AUROC all [0.8444444444444446, 1.0, 0.9310344827586207, 0.9166666666666667, 0.9080459770114943]\n",
      "epoc val\n",
      "AUROC val 0.9307151979565772\n",
      "AUROC all [0.8518518518518519, 1.0, 0.9310344827586207, 0.9166666666666667, 0.9540229885057471]\n",
      "Loss:0.4676589369773865\n",
      "epoc val\n",
      "AUROC val 0.9307151979565772\n",
      "AUROC all [0.8518518518518519, 1.0, 0.9310344827586207, 0.9166666666666667, 0.9540229885057471]\n",
      "epoc val\n",
      "AUROC val 0.9325670498084291\n",
      "AUROC all [0.8444444444444444, 1.0, 0.9310344827586207, 0.9333333333333333, 0.9540229885057471]\n",
      "Loss:0.45537543296813965\n",
      "epoc val\n",
      "AUROC val 0.9325670498084291\n",
      "AUROC all [0.8444444444444444, 1.0, 0.9310344827586207, 0.9333333333333333, 0.9540229885057471]\n",
      "epoc val\n",
      "AUROC val 0.9392337164750957\n",
      "AUROC all [0.8444444444444444, 1.0, 0.9310344827586207, 0.9666666666666667, 0.9540229885057471]\n",
      "Loss:0.44485044479370117\n",
      "epoc val\n",
      "AUROC val 0.9392337164750957\n",
      "AUROC all [0.8444444444444444, 1.0, 0.9310344827586207, 0.9666666666666667, 0.9540229885057471]\n",
      "epoc val\n",
      "AUROC val 0.9377522349936143\n",
      "AUROC all [0.837037037037037, 1.0, 0.9310344827586207, 0.9666666666666667, 0.9540229885057471]\n",
      "Loss:0.4373224973678589\n",
      "epoc val\n",
      "AUROC val 0.9377522349936143\n",
      "AUROC all [0.837037037037037, 1.0, 0.9310344827586207, 0.9666666666666667, 0.9540229885057471]\n",
      "epoc val\n",
      "AUROC val 0.9402681992337165\n",
      "AUROC all [0.8444444444444444, 1.0, 0.9310344827586207, 0.9833333333333334, 0.9425287356321839]\n",
      "Loss:0.43183043599128723\n",
      "epoc val\n",
      "AUROC val 0.9402681992337165\n",
      "AUROC all [0.8444444444444444, 1.0, 0.9310344827586207, 0.9833333333333334, 0.9425287356321839]\n",
      "epoc val\n",
      "AUROC val 0.9402681992337165\n",
      "AUROC all [0.8444444444444444, 1.0, 0.9310344827586207, 0.9833333333333334, 0.9425287356321839]\n",
      "Loss:0.4279639720916748\n",
      "epoc val\n",
      "AUROC val 0.9402681992337165\n",
      "AUROC all [0.8444444444444444, 1.0, 0.9310344827586207, 0.9833333333333334, 0.9425287356321839]\n",
      "epoc val\n",
      "AUROC val 0.9471647509578544\n",
      "AUROC all [0.8444444444444446, 1.0, 0.9540229885057471, 0.9833333333333334, 0.9540229885057471]\n",
      "Loss:0.4242148697376251\n",
      "epoc val\n",
      "AUROC val 0.9471647509578544\n",
      "AUROC all [0.8444444444444446, 1.0, 0.9540229885057471, 0.9833333333333334, 0.9540229885057471]\n",
      "epoc val\n",
      "AUROC val 0.9471647509578544\n",
      "AUROC all [0.8444444444444444, 1.0, 0.9540229885057471, 0.9833333333333334, 0.9540229885057471]\n",
      "Loss:0.42087793350219727\n",
      "epoc val\n",
      "AUROC val 0.9471647509578544\n",
      "AUROC all [0.8444444444444444, 1.0, 0.9540229885057471, 0.9833333333333334, 0.9540229885057471]\n",
      "epoc val\n",
      "AUROC val 0.9393869731800766\n",
      "AUROC all [0.8222222222222222, 1.0, 0.9540229885057471, 0.9666666666666667, 0.9540229885057471]\n",
      "Loss:0.4173564910888672\n",
      "epoc val\n",
      "AUROC val 0.9393869731800766\n",
      "AUROC all [0.8222222222222222, 1.0, 0.9540229885057471, 0.9666666666666667, 0.9540229885057471]\n",
      "epoc val\n",
      "AUROC val 0.9442017879948914\n",
      "AUROC all [0.8296296296296297, 1.0, 0.9540229885057471, 0.9833333333333334, 0.9540229885057471]\n",
      "Loss:0.4155350923538208\n",
      "epoc val\n",
      "AUROC val 0.9442017879948914\n",
      "AUROC all [0.8296296296296297, 1.0, 0.9540229885057471, 0.9833333333333334, 0.9540229885057471]\n",
      "epoc val\n",
      "AUROC val 0.9442017879948914\n",
      "AUROC all [0.8296296296296296, 1.0, 0.9540229885057471, 0.9833333333333334, 0.9540229885057471]\n",
      "Loss:0.4131971597671509\n",
      "epoc val\n",
      "AUROC val 0.9442017879948914\n",
      "AUROC all [0.8296296296296296, 1.0, 0.9540229885057471, 0.9833333333333334, 0.9540229885057471]\n",
      "epoc val\n",
      "AUROC val 0.9446487867177522\n",
      "AUROC all [0.837037037037037, 1.0, 0.9655172413793103, 0.9666666666666667, 0.9540229885057471]\n",
      "Loss:0.4121951162815094\n",
      "epoc val\n",
      "AUROC val 0.9446487867177522\n",
      "AUROC all [0.837037037037037, 1.0, 0.9655172413793103, 0.9666666666666667, 0.9540229885057471]\n",
      "epoc val\n",
      "AUROC val 0.9471647509578544\n",
      "AUROC all [0.8444444444444444, 1.0, 0.9540229885057471, 0.9833333333333334, 0.9540229885057471]\n",
      "Loss:0.41018232703208923\n",
      "epoc val\n",
      "AUROC val 0.9471647509578544\n",
      "AUROC all [0.8444444444444444, 1.0, 0.9540229885057471, 0.9833333333333334, 0.9540229885057471]\n",
      "epoc val\n",
      "AUROC val 0.9393869731800766\n",
      "AUROC all [0.8222222222222222, 1.0, 0.9540229885057471, 0.9666666666666667, 0.9540229885057471]\n",
      "Loss:0.40925559401512146\n",
      "epoc val\n",
      "AUROC val 0.9393869731800766\n",
      "AUROC all [0.8222222222222222, 1.0, 0.9540229885057471, 0.9666666666666667, 0.9540229885057471]\n",
      "epoc val\n",
      "AUROC val 0.9502809706257981\n",
      "AUROC all [0.837037037037037, 1.0, 0.9770114942528736, 0.9833333333333334, 0.9540229885057471]\n",
      "Loss:0.40756428241729736\n",
      "epoc val\n",
      "AUROC val 0.9502809706257981\n",
      "AUROC all [0.837037037037037, 1.0, 0.9770114942528736, 0.9833333333333334, 0.9540229885057471]\n",
      "epoc val\n",
      "AUROC val 0.9465006385696041\n",
      "AUROC all [0.8296296296296296, 1.0, 0.9655172413793104, 0.9833333333333334, 0.9540229885057471]\n",
      "Loss:0.40570542216300964\n",
      "epoc val\n",
      "AUROC val 0.9465006385696041\n",
      "AUROC all [0.8296296296296296, 1.0, 0.9655172413793104, 0.9833333333333334, 0.9540229885057471]\n",
      "epoc val\n",
      "AUROC val 0.9473180076628352\n",
      "AUROC all [0.8222222222222222, 1.0, 0.9770114942528736, 0.9833333333333334, 0.9540229885057471]\n",
      "Loss:0.4042019546031952\n",
      "epoc val\n",
      "AUROC val 0.9473180076628352\n",
      "AUROC all [0.8222222222222222, 1.0, 0.9770114942528736, 0.9833333333333334, 0.9540229885057471]\n",
      "epoc val\n",
      "AUROC val 0.9416858237547892\n",
      "AUROC all [0.8222222222222222, 1.0, 0.9655172413793103, 0.9666666666666667, 0.9540229885057471]\n",
      "Loss:0.403024286031723\n",
      "epoc val\n",
      "AUROC val 0.9416858237547892\n",
      "AUROC all [0.8222222222222222, 1.0, 0.9655172413793103, 0.9666666666666667, 0.9540229885057471]\n",
      "epoc val\n",
      "AUROC val 0.9486462324393358\n",
      "AUROC all [0.8518518518518519, 1.0, 0.9540229885057471, 0.9833333333333334, 0.9540229885057471]\n",
      "Loss:0.40254127979278564\n",
      "epoc val\n",
      "AUROC val 0.9486462324393358\n",
      "AUROC all [0.8518518518518519, 1.0, 0.9540229885057471, 0.9833333333333334, 0.9540229885057471]\n",
      "epoc val\n",
      "AUROC val 0.9547254150702426\n",
      "AUROC all [0.8592592592592592, 1.0, 0.9770114942528736, 0.9833333333333334, 0.9540229885057471]\n",
      "Loss:0.4019363820552826\n",
      "epoc val\n",
      "AUROC val 0.9547254150702426\n",
      "AUROC all [0.8592592592592592, 1.0, 0.9770114942528736, 0.9833333333333334, 0.9540229885057471]\n",
      "epoc val\n",
      "AUROC val 0.9487994891443167\n",
      "AUROC all [0.8296296296296297, 1.0, 0.9655172413793104, 0.9833333333333334, 0.9655172413793104]\n",
      "Loss:0.40083378553390503\n",
      "epoc val\n",
      "AUROC val 0.9487994891443167\n",
      "AUROC all [0.8296296296296297, 1.0, 0.9655172413793104, 0.9833333333333334, 0.9655172413793104]\n",
      "epoc val\n",
      "AUROC val 0.9465006385696041\n",
      "AUROC all [0.8296296296296297, 1.0, 0.9655172413793104, 0.9833333333333334, 0.9540229885057471]\n",
      "Epoch [1] [save] [02062019-045318] loss= 0.20762303471565247\n",
      "epoc val\n",
      "AUROC val 0.94272030651341\n",
      "AUROC all [0.8222222222222222, 1.0, 0.9540229885057471, 0.9833333333333334, 0.9540229885057471]\n",
      "Loss:0.48036596179008484\n",
      "epoc val\n",
      "AUROC val 0.94272030651341\n",
      "AUROC all [0.8222222222222222, 1.0, 0.9540229885057471, 0.9833333333333334, 0.9540229885057471]\n",
      "epoc val\n",
      "AUROC val 0.9404214559386972\n",
      "AUROC all [0.8222222222222222, 1.0, 0.9425287356321839, 0.9833333333333334, 0.9540229885057471]\n",
      "Loss:0.3845544159412384\n",
      "epoc val\n",
      "AUROC val 0.9404214559386972\n",
      "AUROC all [0.8222222222222222, 1.0, 0.9425287356321839, 0.9833333333333334, 0.9540229885057471]\n",
      "epoc val\n",
      "AUROC val 0.94272030651341\n",
      "AUROC all [0.8222222222222223, 1.0, 0.9540229885057471, 0.9833333333333334, 0.9540229885057471]\n",
      "Loss:0.37705913186073303\n",
      "epoc val\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC val 0.94272030651341\n",
      "AUROC all [0.8222222222222223, 1.0, 0.9540229885057471, 0.9833333333333334, 0.9540229885057471]\n",
      "epoc val\n",
      "AUROC val 0.9412388250319286\n",
      "AUROC all [0.8148148148148148, 1.0, 0.9540229885057471, 0.9833333333333334, 0.9540229885057471]\n",
      "Loss:0.3746984899044037\n",
      "epoc val\n",
      "AUROC val 0.9412388250319286\n",
      "AUROC all [0.8148148148148148, 1.0, 0.9540229885057471, 0.9833333333333334, 0.9540229885057471]\n",
      "epoc val\n",
      "AUROC val 0.9400510855683268\n",
      "AUROC all [0.8370370370370371, 1.0, 0.9425287356321839, 0.9666666666666667, 0.9540229885057471]\n",
      "Loss:0.3750905394554138\n",
      "epoc val\n",
      "AUROC val 0.9400510855683268\n",
      "AUROC all [0.8370370370370371, 1.0, 0.9425287356321839, 0.9666666666666667, 0.9540229885057471]\n",
      "epoc val\n",
      "AUROC val 0.9356066411238825\n",
      "AUROC all [0.8148148148148149, 1.0, 0.9425287356321839, 0.9666666666666667, 0.9540229885057471]\n",
      "Loss:0.3784293532371521\n",
      "epoc val\n",
      "AUROC val 0.9356066411238825\n",
      "AUROC all [0.8148148148148149, 1.0, 0.9425287356321839, 0.9666666666666667, 0.9540229885057471]\n",
      "epoc val\n",
      "AUROC val 0.9397573435504469\n",
      "AUROC all [0.8074074074074076, 1.0, 0.9540229885057471, 0.9833333333333334, 0.9540229885057471]\n",
      "Loss:0.3783813714981079\n",
      "epoc val\n",
      "AUROC val 0.9397573435504469\n",
      "AUROC all [0.8074074074074076, 1.0, 0.9540229885057471, 0.9833333333333334, 0.9540229885057471]\n",
      "epoc val\n",
      "AUROC val 0.9476117496807153\n",
      "AUROC all [0.8518518518518519, 1.0, 0.9655172413793104, 0.9666666666666667, 0.9540229885057471]\n",
      "Loss:0.37873637676239014\n",
      "epoc val\n",
      "AUROC val 0.9476117496807153\n",
      "AUROC all [0.8518518518518519, 1.0, 0.9655172413793104, 0.9666666666666667, 0.9540229885057471]\n",
      "epoc val\n",
      "AUROC val 0.9402043422733077\n",
      "AUROC all [0.8148148148148148, 1.0, 0.9655172413793104, 0.9666666666666667, 0.9540229885057471]\n",
      "Loss:0.3783057630062103\n",
      "epoc val\n",
      "AUROC val 0.9402043422733077\n",
      "AUROC all [0.8148148148148148, 1.0, 0.9655172413793104, 0.9666666666666667, 0.9540229885057471]\n",
      "epoc val\n",
      "AUROC val 0.9416858237547892\n",
      "AUROC all [0.8222222222222222, 1.0, 0.9655172413793104, 0.9666666666666667, 0.9540229885057471]\n",
      "Loss:0.3789633512496948\n",
      "epoc val\n",
      "AUROC val 0.9416858237547892\n",
      "AUROC all [0.8222222222222222, 1.0, 0.9655172413793104, 0.9666666666666667, 0.9540229885057471]\n",
      "epoc val\n",
      "AUROC val 0.9442017879948914\n",
      "AUROC all [0.8296296296296296, 1.0, 0.9540229885057471, 0.9833333333333334, 0.9540229885057471]\n",
      "Loss:0.37894880771636963\n",
      "epoc val\n",
      "AUROC val 0.9442017879948914\n",
      "AUROC all [0.8296296296296296, 1.0, 0.9540229885057471, 0.9833333333333334, 0.9540229885057471]\n",
      "epoc val\n",
      "AUROC val 0.9420561941251597\n",
      "AUROC all [0.8074074074074075, 1.0, 0.9655172413793104, 0.9833333333333334, 0.9540229885057471]\n",
      "Loss:0.38109976053237915\n",
      "epoc val\n",
      "AUROC val 0.9420561941251597\n",
      "AUROC all [0.8074074074074075, 1.0, 0.9655172413793104, 0.9833333333333334, 0.9540229885057471]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c13a5860fbcf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0mlaunchTimestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m \u001b[0mChexnetTrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTRAIN_IMAGE_LIST\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mVAL_IMAGE_LIST\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnnArchitecture\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnnIsTrained\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnnClassCount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrBatchSize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrMaxEpoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransResize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransCrop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlaunchTimestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-c13a5860fbcf>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(pathDirData, pathFileTrain, pathFileVal, nnArchitecture, nnIsTrained, nnClassCount, trBatchSize, trMaxEpoch, transResize, transCrop, launchTimestamp, checkpoint, classes)\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mtimestampSTART\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimestampDate\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'-'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimestampTime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m             \u001b[0mlossTrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChexnetTrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochTrain\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataLoaderTrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataLoaderVal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrMaxEpoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnnClassCount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m             \u001b[0mlossVal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosstensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChexnetTrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochVal\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataLoaderVal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrMaxEpoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnnClassCount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-c13a5860fbcf>\u001b[0m in \u001b[0;36mepochTrain\u001b[0;34m(model, dataLoader, dataLoaderVal, optimizer, scheduler, epochMax, classCount, loss, counter, classes)\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0mvarOutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvarOutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m             \u001b[0mlossvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvarOutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvarTarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m             \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mavg_loss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatchID\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatchID\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlossvalue\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatchID\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-c13a5860fbcf>\u001b[0m in \u001b[0;36mlossCriterion\u001b[0;34m(varOutput, varTarget)\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0mBCEloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBCELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             \u001b[0mL1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBCEloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvarOutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvarTarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m             \u001b[0mL2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBCEloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvarOutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvarTarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0mL3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBCEloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvarOutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvarTarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    510\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2112\u001b[0m     return torch._C._nn.binary_cross_entropy(\n\u001b[0;32m-> 2113\u001b[0;31m         input, target, weight, reduction_enum)\n\u001b[0m\u001b[1;32m   2114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as tfunc\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn.functional as func\n",
    "\n",
    "from sklearn.metrics.ranking import roc_auc_score\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from models.chexnet.DensenetModels import DenseNet121\n",
    "from models.models import ResNet18\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "writer = SummaryWriter('./logs')\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Read images and corresponding labels.\n",
    "\"\"\"\n",
    "class ChestXrayDataSet(Dataset):\n",
    "    \n",
    "    def convert_to_ones(self, df, disease):\n",
    "        df[disease] = df[disease].replace([-1.0], 1.0)\n",
    "    \n",
    "    def convert_to_zeros(self, df, disease):\n",
    "        df[disease] = df[disease].replace([-1.0], 0.0)\n",
    "        \n",
    "    def convert_to_multi(self, df, disease):\n",
    "        df[disease] = df[disease].replace([-1.0], 2.0)\n",
    "\n",
    "    def __init__(self, data_dir, image_list_file, diseases=['Atelectasis', 'Consolidation', 'Edema','Cardiomegaly', 'Pleural Effusion'], side='Frontal', transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_dir: path to image directory.\n",
    "            image_list_file: path to the file containing images\n",
    "                with corresponding labels.\n",
    "            transform: optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        image_names = []\n",
    "        labels = []\n",
    "        chex_df = pd.read_csv(image_list_file)\n",
    "        chex_df = chex_df.fillna(0.0)\n",
    "#         if side == 'Hybrid':\n",
    "#             good_lat_indices = []\n",
    "#             paths = chex_df['Path']\n",
    "#             for i in range(1, len(paths)):\n",
    "#                 path_lat = paths[i]\n",
    "#                 if 'lateral' in path_lat:\n",
    "#                     if path_lat.replace('2_lateral', '1_frontal') in paths[i-1]:\n",
    "#                         good_lat_indices.append(i)\n",
    "#             chex_df = chex_df.iloc[good_lat_indices, :]  \n",
    "#         else:\n",
    "#            chex_df = chex_df.loc[chex_df['Frontal/Lateral'] == 'Lateral']\n",
    "    \n",
    "        if side == 'Hybrid':\n",
    "            good_lat_indices = []\n",
    "            paths = chex_df['Path']\n",
    "            for i in range(1, len(paths)):\n",
    "                path_lat = paths[i]\n",
    "                if 'lateral' in path_lat:\n",
    "                    if os.path.isfile('./data/' + path_lat.replace('2_lateral', '1_frontal')):\n",
    "                        good_lat_indices.append(i)\n",
    "            chex_df = chex_df.iloc[good_lat_indices, :] \n",
    "        else:\n",
    "            chex_df = chex_df.loc[chex_df['Frontal/Lateral'] == side]\n",
    "        self.convert_to_ones(chex_df, 'Atelectasis')\n",
    "        self.convert_to_ones(chex_df, 'Consolidation')\n",
    "        self.convert_to_ones(chex_df, 'Edema')\n",
    "        self.convert_to_multi(chex_df, 'Cardiomegaly')\n",
    "        self.convert_to_multi(chex_df, 'Pleural Effusion')\n",
    "\n",
    "#         chex_df_diseases = chex_df[diseases]\n",
    "                         \n",
    "#         if 'train' in image_list_file:\n",
    "#             chex_df = chex_df\n",
    "#         if len(diseases) == 1:\n",
    "#             chex_df = chex_df.loc[chex_df['Pleural Effusion'] != -1] #U-Ignore\n",
    "#         print(chex_df)\n",
    "        labels = chex_df.as_matrix(columns=diseases)\n",
    "        labels = list(labels)\n",
    "\n",
    "        image_names = chex_df.as_matrix(columns=['Path']).flatten()\n",
    "        image_names = [os.path.join(data_dir, im_name) for im_name in image_names]\n",
    "\n",
    "        self.image_names = image_names\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index: the index of item\n",
    "        Returns:\n",
    "            image and its labels\n",
    "        \"\"\"\n",
    "        side_image_name = self.image_names[index]\n",
    "        front_image_name = side_image_name.replace('2_lateral', '1_frontal')\n",
    "        side_image = Image.open(side_image_name).convert('RGB')\n",
    "        front_image = Image.open(front_image_name).convert('RGB')\n",
    "        label = torch.FloatTensor(self.labels[index])\n",
    "        if self.transform is not None:\n",
    "            side_image = self.transform(side_image)\n",
    "            front_image = self.transform(front_image)\n",
    "        return front_image, side_image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "class ChexnetTrainer():\n",
    "\n",
    "    #---- Train the densenet network \n",
    "    #---- pathDirData - path to the directory that contains images\n",
    "    #---- pathFileTrain - path to the file that contains image paths and label pairs (training set)\n",
    "    #---- pathFileVal - path to the file that contains image path and label pairs (validation set)\n",
    "    #---- nnArchitecture - model architecture 'DENSE-NET-121', 'DENSE-NET-169' or 'DENSE-NET-201'\n",
    "    #---- nnIsTrained - if True, uses pre-trained version of the network (pre-trained on imagenet)\n",
    "    #---- nnClassCount - number of output classes \n",
    "    #---- trBatchSize - batch size\n",
    "    #---- trMaxEpoch - number of epochs\n",
    "    #---- transResize - size of the image to scale down to (not used in current implementation)\n",
    "    #---- transCrop - size of the cropped image \n",
    "    #---- launchTimestamp - date/time, used to assign unique name for the checkpoint file\n",
    "    #---- checkpoint - if not None loads the model and continues training\n",
    "    #--- classes - is the number of classes to predict (Note =/= final layer of Dense Net) -- Saj\n",
    "    \n",
    "    \n",
    "    def train (pathDirData, pathFileTrain, pathFileVal, nnArchitecture, nnIsTrained, nnClassCount, trBatchSize, trMaxEpoch, transResize, transCrop, launchTimestamp, checkpoint,classes):\n",
    "        #------------------  Special Loss \n",
    "        # Takes in Logits, except 0,1,2 --> logits => sigmoid\n",
    "        # returns multi label loss\n",
    "        def lossCriterion(varOutput,varTarget):\n",
    "            CEloss =  torch.nn.CrossEntropyLoss()\n",
    "            BCEloss = torch.nn.BCELoss()\n",
    "\n",
    "            L1 = BCEloss(varOutput[:,0],varTarget[:,0]) \n",
    "            L2 = BCEloss(varOutput[:,1],varTarget[:,1])\n",
    "            L3 = BCEloss(varOutput[:,2],varTarget[:,2])\n",
    "            varTarget = varTarget.long()\n",
    "            L4 = CEloss(varOutput[:,3:6],varTarget[:,3])\n",
    "            L5 = CEloss(varOutput[:,6:9],varTarget[:,4])\n",
    "\n",
    "            \n",
    "            lossvalue = (L1 + L2 + L3 + L4 + L5)/5\n",
    "            \n",
    "            return lossvalue\n",
    "        \n",
    "        #--------------------Settings: best models for side-forward\n",
    "        Fcheckpoint = './best_models/forward121/m-14340_0-Copy1.891.pth.tar'\n",
    "        Scheckpoint = './best_models/lateral/m-5065_0.874.pth.tar'\n",
    "        \n",
    "        #-------------------- SETTINGS: NETWORK ARCHITECTURE\n",
    "        if nnArchitecture == 'DENSE-NET-121': model = DenseNet121(nnClassCount, nnIsTrained).cuda()\n",
    "        elif nnArchitecture == 'DENSE-NET-169': model = DenseNet169(nnClassCount, nnIsTrained).cuda()\n",
    "        elif nnArchitecture == 'DENSE-NET-201': model = DenseNet201(nnClassCount, nnIsTrained).cuda()\n",
    "        elif nnArchitecture == 'RES-NET-18': model = ResNet18(nnClassCount, nnIsTrained).cuda()\n",
    "        elif nnArchitecture == 'side-forward': model = forward_side(nnClassCount,Fcheckpoint,Scheckpoint)\n",
    "        model = torch.nn.DataParallel(model).cuda()\n",
    "       \n",
    "        #-------------------- SETTINGS: DATA TRANSFORMS |TRAIN|\n",
    "        normalize = transforms.Normalize([0.50616586, 0.50616586, 0.50616586], [0.2879059, 0.2879059, 0.2879059]) #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        \n",
    "        transformList = []\n",
    "        transformList.append(transforms.Resize(transResize))\n",
    "        transformList.append(transforms.ToTensor())\n",
    "        transformList.append(normalize)    \n",
    "        transformSequence=transforms.Compose(transformList)\n",
    "\n",
    "        #-------------------- SETTINGS: DATASET BUILDER |TRAIN|\n",
    "                    \n",
    "        datasetTrain = ChestXrayDataSet(data_dir=pathDirData,image_list_file=pathFileTrain, side='Hybrid', transform=transformSequence)              \n",
    "        dataLoaderTrain = DataLoader(dataset=datasetTrain, batch_size=trBatchSize, shuffle=True,  num_workers=0, pin_memory=False)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #-------------------- SETTINGS: DATA TRANSFORMS, TEN CROPS |VAL|\n",
    "\n",
    "        \n",
    "        #-------------------- SETTINGS: DATASET BUILDERS |VAL|\n",
    "        datasetVal =   ChestXrayDataSet(data_dir=pathDirData, image_list_file=pathFileVal, side='Hybrid', transform=transformSequence)\n",
    "        dataLoaderVal = DataLoader(dataset=datasetVal, batch_size=trBatchSize, shuffle=False, num_workers=0, pin_memory=False)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #-------------------- SETTINGS: OPTIMIZER & SCHEDULER\n",
    "        optimizer = optim.Adam (model.parameters(), lr=0.0001, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-5)\n",
    "        scheduler = ReduceLROnPlateau(optimizer, factor = 0.1, patience = 5, mode = 'min')\n",
    "\n",
    "        #-------------------- SETTINGS: LOSS\n",
    "        loss = lossCriterion\n",
    "       \n",
    "        counter = 0\n",
    "        \n",
    "\t#---- Load checkpoint \n",
    "        if checkpoint != None:\n",
    "            modelCheckpoint = torch.load(checkpoint)\n",
    "            model.load_state_dict(modelCheckpoint['state_dict'])\n",
    "            optimizer.load_state_dict(modelCheckpoint['optimizer'])\n",
    "            counter = modelCheckpoint['counter']\n",
    "        \n",
    "        #---- TRAIN THE NETWORK\n",
    "        lossMIN = 100000\n",
    "        \n",
    "        for epochID in range (0, trMaxEpoch):\n",
    "            \n",
    "            timestampTime = time.strftime(\"%H%M%S\")\n",
    "            timestampDate = time.strftime(\"%d%m%Y\")\n",
    "            timestampSTART = timestampDate + '-' + timestampTime\n",
    "                         \n",
    "            lossTrain, counter = ChexnetTrainer.epochTrain (model, dataLoaderTrain, dataLoaderVal, optimizer, scheduler, trMaxEpoch, nnClassCount, loss, counter,classes)\n",
    "            lossVal, losstensor, __ = ChexnetTrainer.epochVal (model, dataLoaderVal, optimizer, scheduler, trMaxEpoch, nnClassCount, loss, counter,classes)\n",
    "            \n",
    "            timestampTime = time.strftime(\"%H%M%S\")\n",
    "            timestampDate = time.strftime(\"%d%m%Y\")\n",
    "            timestampEND = timestampDate + '-' + timestampTime\n",
    "\n",
    "            scheduler.step(losstensor.item())\n",
    "            writer.add_scalar('logs/train_loss_epoch', lossTrain, epochID)\n",
    "            writer.add_scalar('logs/val_loss_epoch', lossVal, epochID)\n",
    "            if lossVal < lossMIN:\n",
    "\n",
    "                lossMIN = lossVal    \n",
    "                torch.save({'epoch': epochID + 1, 'state_dict': model.state_dict(), 'best_loss': lossMIN, 'optimizer' : optimizer.state_dict()}, './different_hybrid/m-' + launchTimestamp + '.pth.tar')\n",
    "                print ('Epoch [' + str(epochID + 1) + '] [save] [' + timestampEND + '] loss= ' + str(lossVal))\n",
    "            else:\n",
    "                print ('Epoch [' + str(epochID + 1) + '] [----] [' + timestampEND + '] loss= ' + str(lossVal))\n",
    "                     \n",
    "    #-------------------------------------------------------------------------------- \n",
    "       \n",
    "    def epochTrain (model, dataLoader, dataLoaderVal, optimizer, scheduler, epochMax, classCount, loss, counter,classes):\n",
    "        \n",
    "        model.train()\n",
    "        lossTrain = 0\n",
    "        lossTrainNorm = 0\n",
    "        \n",
    "        avg_loss = 0.0\n",
    "\n",
    "        for batchID, (input1, input2, target) in enumerate (dataLoader):\n",
    "\n",
    "            target = target.cuda()\n",
    "            varInput1 = torch.autograd.Variable(input1)\n",
    "            varInput2 = torch.autograd.Variable(input2)\n",
    "            varTarget = torch.autograd.Variable(target)         \n",
    "            varOutput = model(varInput1,varInput2)\n",
    "\n",
    "\n",
    "            varOutput[:,0] = torch.sigmoid(varOutput[:,0])\n",
    "            varOutput[:,1] = torch.sigmoid(varOutput[:,1])\n",
    "            varOutput[:,2] = torch.sigmoid(varOutput[:,2])\n",
    "\n",
    "            lossvalue = loss(varOutput,varTarget)\n",
    "\n",
    "            avg_loss = avg_loss * (batchID)/(batchID+1) + lossvalue * 1.0/(batchID+ 1)\n",
    "            lossTrain += lossvalue\n",
    "            lossTrainNorm += 1\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            lossvalue.backward()\n",
    "            optimizer.step()\n",
    "            writer.add_scalar('logs/train_loss', avg_loss, counter)\n",
    "            if batchID % 41 == 0:\n",
    "                ChexnetTrainer.epochVal(model, dataLoaderVal, optimizer, scheduler, epochMax, classCount, loss, counter,classes)\n",
    "                print('Loss:' + str(avg_loss.item()))\n",
    "            if batchID % 41 == 0:\n",
    "                __, __, aurocMean = ChexnetTrainer.epochVal(model, dataLoaderVal, optimizer, scheduler, epochMax, classCount, loss, counter,classes)\n",
    "                torch.save({'counter' : counter, 'state_dict': model.state_dict(), 'valAUROC' : aurocMean , 'optimizer' : optimizer.state_dict()}, './different_hybrid/m-' + str(counter) + '_' + str(round(aurocMean, 3)) + '.pth.tar')\n",
    "\n",
    "                \n",
    "#             print(counter)\n",
    "            counter += 1\n",
    "\n",
    "        outLoss = lossTrain/lossTrainNorm\n",
    "        return outLoss, counter\n",
    "\n",
    "                        \n",
    "    #-------------------------------------------------------------------------------- \n",
    "        \n",
    "    def epochVal (model, dataLoader, optimizer, scheduler, epochMax, classCount, loss, counter,classes):\n",
    "        \n",
    "        print('epoc val')\n",
    "        model.eval()\n",
    "        \n",
    "        lossVal = 0\n",
    "        lossValNorm = 0\n",
    "        losstensorMean = 0\n",
    "\n",
    "        outGT = torch.FloatTensor().cuda()\n",
    "        outPRED = torch.FloatTensor().cuda()\n",
    "        with torch.no_grad():\n",
    "            for i, (input1, input2, target) in enumerate(dataLoader):\n",
    "                #Val code\n",
    "                target = target.cuda()\n",
    "                varInput1 = torch.autograd.Variable(input1).cuda()\n",
    "                varInput2 = torch.autograd.Variable(input2).cuda()\n",
    "                varTarget = torch.autograd.Variable(target)\n",
    "                varOutput = model(varInput1,varInput2)\n",
    "\n",
    "                varOutput[:,0] = torch.sigmoid(varOutput[:,0])\n",
    "                varOutput[:,1] = torch.sigmoid(varOutput[:,1])\n",
    "                varOutput[:,2] = torch.sigmoid(varOutput[:,2])            \n",
    "\n",
    "\n",
    "                ### VAL Preds for AUROC\n",
    "                bPRED = torch.zeros(varOutput.shape[0], 5).cuda()\n",
    "                bPRED[:,0] = varOutput[:,0]\n",
    "                bPRED[:,1] = varOutput[:,1]\n",
    "                bPRED[:,2] = varOutput[:,2]\n",
    "                \n",
    "                soft_a = torch.nn.functional.softmax(varOutput[:,3:6], dim=-1).data\n",
    "\n",
    "                a0, a1, a2 = soft_a[:, 0], soft_a[:, 1], soft_a[:, 2]\n",
    "                bPRED[:, 3] = a1/(a0+a1)\n",
    "                soft_b = torch.nn.functional.softmax(varOutput[:,6:9], dim=-1).data\n",
    "                b0, b1, b2 = soft_b[:, 0], soft_b[:, 1], soft_b[:, 2]\n",
    "                bPRED[:, 4] = b1/(b0+b1)\n",
    "\n",
    "                outPRED = torch.cat((outPRED, bPRED.data), 0)            \n",
    "                outGT = torch.cat((outGT, target), 0)\n",
    "\n",
    "\n",
    "                losstensor = loss(varOutput,varTarget)\n",
    "\n",
    "                losstensorMean += losstensor\n",
    "                lossVal += losstensor.item()\n",
    "                lossValNorm += 1\n",
    "                ##block comment was here\n",
    "\n",
    "            outLoss = lossVal / lossValNorm\n",
    "            losstensorMean = losstensorMean / lossValNorm\n",
    "\n",
    "            aurocIndividual = ChexnetTrainer.computeAUROC(outGT, outPRED, classes)\n",
    "            aurocMean = np.array(aurocIndividual).mean()\n",
    "\n",
    "            print(\"AUROC val\", aurocMean)\n",
    "            print(\"AUROC all\", aurocIndividual)\n",
    "            writer.add_scalar('logs/val_auroc', aurocMean, counter)\n",
    "\n",
    "        return outLoss, losstensorMean, aurocMean            \n",
    "\n",
    "\n",
    "               \n",
    "    #--------------------------------------------------------------------------------     \n",
    "     \n",
    "    #---- Computes area under ROC curve \n",
    "    #---- dataGT - ground truth data\n",
    "    #---- dataPRED - predicted data\n",
    "    #---- classCount - number of classes\n",
    "    \n",
    "    def computeAUROC (dataGT, dataPRED, classCount):\n",
    "        \n",
    "        outAUROC = []\n",
    "        \n",
    "        datanpGT = dataGT.cpu().numpy()\n",
    "        datanpPRED = dataPRED.cpu().numpy()\n",
    "        \n",
    "        for i in range(classCount):\n",
    "            outAUROC.append(roc_auc_score(datanpGT[:, i], datanpPRED[:, i]))\n",
    "            \n",
    "        return outAUROC\n",
    "        \n",
    "        \n",
    "    #--------------------------------------------------------------------------------  \n",
    "    \n",
    "    #---- Test the trained network \n",
    "    #---- pathDirData - path to the directory that contains images\n",
    "    #---- pathFileTrain - path to the file that contains image paths and label pairs (training set)\n",
    "    #---- pathFileVal - path to the file that contains image path and label pairs (validation set)\n",
    "    #---- nnArchitecture - model architecture 'DENSE-NET-121', 'DENSE-NET-169' or 'DENSE-NET-201'\n",
    "    #---- nnIsTrained - if True, uses pre-trained version of the network (pre-trained on imagenet)\n",
    "    #---- nnClassCount - number of output classes \n",
    "    #---- trBatchSize - batch size\n",
    "    #---- trMaxEpoch - number of epochs\n",
    "    #---- transResize - size of the image to scale down to (not used in current implementation)\n",
    "    #---- transCrop - size of the cropped image \n",
    "    #---- launchTimestamp - date/time, used to assign unique name for the checkpoint file\n",
    "    #---- checkpoint - if not None loads the model and continues training\n",
    "    \n",
    "    def test (pathDirData, pathFileTest, pathModel, nnArchitecture, nnClassCount, nnIsTrained, trBatchSize, transResize, transCrop, launchTimeStamp):   \n",
    "        \n",
    "        \n",
    "        CLASS_NAMES = [ 'Atelectasis', 'Cardiomegaly', 'Effusion', 'Infiltration', 'Mass', 'Nodule', 'Pneumonia',\n",
    "                'Pneumothorax', 'Consolidation', 'Edema', 'Emphysema', 'Fibrosis', 'Pleural_Thickening', 'Hernia']\n",
    "        \n",
    "        cudnn.benchmark = True\n",
    "        \n",
    "        #-------------------- SETTINGS: NETWORK ARCHITECTURE, MODEL LOAD\n",
    "        if nnArchitecture == 'DENSE-NET-121': model = DenseNet121(nnClassCount, nnIsTrained).cuda()\n",
    "        elif nnArchitecture == 'DENSE-NET-169': model = DenseNet169(nnClassCount, nnIsTrained).cuda()\n",
    "        elif nnArchitecture == 'DENSE-NET-201': model = DenseNet201(nnClassCount, nnIsTrained).cuda()\n",
    "        \n",
    "        model = torch.nn.DataParallel(model).cuda() \n",
    "        \n",
    "        modelCheckpoint = torch.load(pathModel)\n",
    "        model.load_state_dict(modelCheckpoint['state_dict'])\n",
    "\n",
    "        #-------------------- SETTINGS: DATA TRANSFORMS, TEN CROPS\n",
    "        normalize = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        \n",
    "        #-------------------- SETTINGS: DATASET BUILDERS\n",
    "        transformList = []\n",
    "        transformList.append(transforms.Resize(transResize))\n",
    "        transformList.append(transforms.TenCrop(transCrop))\n",
    "        transformList.append(transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops])))\n",
    "        transformList.append(transforms.Lambda(lambda crops: torch.stack([normalize(crop) for crop in crops])))\n",
    "        transformSequence=transforms.Compose(transformList)\n",
    "        \n",
    "        datasetTest = DatasetGenerator(pathImageDirectory=pathDirData, pathDatasetFile=pathFileTest, transform=transformSequence)\n",
    "        dataLoaderTest = DataLoader(dataset=datasetTest, batch_size=trBatchSize, num_workers=0, shuffle=False, pin_memory=False)\n",
    "        \n",
    "        outGT = torch.FloatTensor().cuda()\n",
    "        outPRED = torch.FloatTensor().cuda()\n",
    "       \n",
    "        model.eval()\n",
    "        \n",
    "        for i, (input, target) in enumerate(dataLoaderTest):\n",
    "            \n",
    "            target = target.cuda()\n",
    "            outGT = torch.cat((outGT, target), 0)\n",
    "            \n",
    "            bs, n_crops, c, h, w = input.size()\n",
    "            \n",
    "            varInput = torch.autograd.Variable(input.view(-1, c, h, w).cuda())\n",
    "            \n",
    "            out = model(varInput)\n",
    "            outMean = out.view(bs, n_crops, -1).mean(1)\n",
    "            \n",
    "            outPRED = torch.cat((outPRED, outMean.data), 0)\n",
    "\n",
    "        aurocIndividual = ChexnetTrainer.computeAUROC(outGT, outPRED, nnClassCount)\n",
    "        aurocMean = np.array(aurocIndividual).mean()\n",
    "        \n",
    "        print ('AUROC mean ', aurocMean)\n",
    "        \n",
    "        for i in range (0, len(aurocIndividual)):\n",
    "            print (CLASS_NAMES[i], ' ', aurocIndividual[i])\n",
    "        \n",
    "     \n",
    "        return\n",
    "#-------------------------------------------------------------------------------- \n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "DATA_DIR = './data'\n",
    "TRAIN_IMAGE_LIST = './data/CheXpert-v1.0-small/train.csv'\n",
    "VAL_IMAGE_LIST = './data/CheXpert-v1.0-small/valid.csv'\n",
    "valid_dataset = ChestXrayDataSet(data_dir=DATA_DIR,\n",
    "                                image_list_file=VAL_IMAGE_LIST)\n",
    "\n",
    "nnIsTrained = True\n",
    "nnArchitecture = 'side-forward'\n",
    "\n",
    "nnClassCount = 9\n",
    "classes = 5\n",
    "\n",
    "trBatchSize = 32\n",
    "trMaxEpoch = 50\n",
    "transResize = (300, 300)\n",
    "transCrop = 224\n",
    "launchTimestamp = ''\n",
    "checkpoint = None\n",
    "ChexnetTrainer.train(DATA_DIR,TRAIN_IMAGE_LIST,VAL_IMAGE_LIST,nnArchitecture, nnIsTrained, nnClassCount, trBatchSize, trMaxEpoch, transResize, transCrop, launchTimestamp, checkpoint,classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save({ 'state_dict': model.state_dict(), 'optimizer' : optimizer.state_dict()}, './m-saj_good_model.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
